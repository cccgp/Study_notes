# 0. 主题
模型压缩 + 遥感图像分类
1. 分类模型 -> 遥感图像
2. 模型压缩 -> 分类网络

# 1. 背景
1. 遥感图像分类领域重要性以及应用。
2. 深度学习兴起，应用于图像分类领域，效果非常好。
3. 但是大型网络模型参数多，计算量大，部署于资源受限设备困难。
4. 采用模型压缩技术应用于大型网络模型。

# 2. 调研
1. 调研现有的模型压缩方法。
    - 现有的方法可以分为五类：
        - 网络剪枝
        - 网络量化
        - 低秩分解
        - 网络蒸馏
        - 紧性网络设计
    - 在此基础上，根据模型压缩过程中利用信息位置的不同可以分为三类：
        - 权值压缩
        - 局部压缩
        - 全局压缩
# 3. 研究思路
1. 初步探究
    - 实验：三种方法 --> 应用于 --> 遥感图像分类的网络模型。
        - 每一种方法选取具有代表性的方法即可。
    - 结果：数据集上的指标。
    - 结论：哪种方法更好？亦或者给出分析，什么场景更加合适使用哪种方案。
    - 最终目的：根据实验亦或者分析给出一种压缩方式用于后续探究。（用于解答问题：对于遥感图像分类，为什么要探究这一类压缩方法？）
2. 对比实验
    - 假设实验结果表明需要深入探究全局压缩方法。
        - 该方法用于网络模型可以得到在数据集上的指标：压缩率等。
        - 上述结果作为实验的baseline。
    - 针对数据集图像加入预处理。
        - 创新点：提出两个。
        - 结果：提升数据集评价指标。
  
# 4. 遥感图像场景分类
以下数据来源于论文：
- Remote Sensing Image Scene Classification: Benchmark and State of the Art [1] - 2017年发表 
- Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities[2] - 2020年发表

## 4.1 数据集

| Datasets | Images per class | Scene classes | Total images | Spatial resolution(m) | Image sizes | Year |
| :---- | :----: | :----: |:----: |:----: |:----: |:----: |
| UC Merced Land-Use | 100 | 21 | 2100 | 0.3 | 256 * 256 | 2010 |
| WHU-RS19 | ~50 |  19 | 1005 | up to 0.5 | 600 * 600 | 2012 | 
| SIRI-WHU | 200| 12 | 2400 | 2 | 200 * 200 | 2016 | 
| RSSCN7 | 400 | 7 | 2800 | -- | 400 * 400 | 2015 | 
| RSC11 | ~100 | 11 | 1232 | 0.2 | 512 * 512 | 2016 |
| Brazilian Coffee Scene | 1438 | 2 | 2876 | -- | 64 * 64 | 2015 |
| NWPU-RESISC45 | 700 | 45 | 31500 | ~30 to 0.2 | 256 * 256 | 2016 |

## 4.2 方法
- 手工特征方法
    - 颜色直方图
    - 纹理特征
    - GIST
    - SIFT
    - HOG
    - so on ...
- 无监督特征学习方法
    - PCA
    - k-means 聚类
    - 稀疏编码
    - 自编码器
    - so on ...
- 深度特征学习方法
    - 深度信念网络 - DBN
    - 深度玻尔兹曼机 - DBM
    - 栈式自编码器 - SAE
    - 卷积神经网络 - CNN
        - AlexNet
        - Overfeat
        - VGGNet
        - GoogLeNet
        - SPPNet
        - ResNet
        - so on ...
    - so on ...
## 4.3 代表性方法在NWPU-RESISC45的结果

| Features | Training ratios | Training ratios | 
| :---- | :----: | :----: |
|  | 10% | 20% |
| AlexNet | 76.69 +/- 0.21 | 79.85 +/- 0.13 |
| VGGNet-16 | 76.47 +/- 0.18 | 79.79 +/- 0.15 |
| GoogLeNet | 76.19 +/- 0.38 | 78.48 +/- 0.26 |

备注：
- 10%：数据集随机切分10%用于训练，90%用于测试
- 20%：数据集随机切分20%用于训练，80%用于测试
- 指标：总体精度（overall accuracy)
## 4.4 复现实验结果

复现了以下几种网络在NWPU-RESISC45数据集上的效果。（使用了在ImageNet上预训练的网络进行微调）

| Features | Training ratios | Training ratios | 
| :---- | :----: | :----: |
|  | 10% | 20% |
| AlexNet |  76.28% |  80.69% |
| VGGNet（VGG-11） | 85.38% | 89.47% |
| squeezenet | 74.76% | 79.44% | 
| resnet | 86.41% | 90.01% |

另外根据[2]内容，一些神经网络模型在该数据集上的效果如下所示：

| Features | Training ratios | Training ratios | 
| :---- | :----: | :----: |
|  | 10% | 20% |
| BoCF | 83.65%  | 84.32% |
| MSCP | 88.07% | 90.81% |
| D-CNNs | 89.22% | 91.89% | 
| IORN | 87.83% | 91.30%|
| ADSSM | 91.69% | 94.29% | 
| SF-CNN | 89.89% | 92.55% | 
| ADFF | 90.58% | 91.91% | 
| CNN-CapsNet | 89.03% | 92.10% | 
| SCCov | 89.30% | 92.10%| 
| Hydra | 92.44% | 94.51% | 

疑问：现在AlexNet等网络可以代表用于遥感图像的深度网络模型吗？虽然在数据集上的表现也非常不错。

解答：关键点是不是在于数据集？只要数据集是遥感图像数据集就可以说明？
1. 原始：AlexNet + NWPU-RESISC45
2. 之后：压缩 + AlexNet + NWPU-RESISC45

# 5. 模型压缩方法
参考论文:
- Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey[1] - 2020年发表
- A Survey of Model Compression and Acceleration for Deep Neural Networks[2] - 2017年发表

大多数压缩方法研究基于AlexNet/VGG16 + ImageNet，最近的工作集中于ResNet/GoogleNet + ImageNet，因此现在的工作应该集中于一个最简单的对比实验，后续实验可以在这方面扩展。
```
1. AlexNet + NWPU-RESISC45
2. 压缩 + AlexNet + NWPU-RESISC45
```

根据[2]的理论：
1. 三个指标：压缩率、加速比和准确率。
2. 一个好的压缩方法：准确率与原模型相同，压缩率与加速比较大。

压缩率 = 原始网络参数数量 / 压缩后网络参数数量

计算模型的参数数量代码
```
def params_count(model):
    """
    Compute the number of parameters.
    Args:
        model (model): model to count the number of parameters.
    """
    return np.sum([p.numel() for p in model.parameters()]).item()
```
## 5.1 权值压缩
参考论文：
- Learning both Weights and Connections for Efficient Neural Networks[1] - 2015年发表
- Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[2] - 2016年发表

具体到权值压缩方法，实验对比方案：
   - AlexNet + NWPU-RESISC45
   - 权值压缩 + AlexNet + NWPU-RESISC45

1. 选定具体权值压缩方法：[1]
2. 实验结果：

压缩率为85%。每一层的详细压缩信息见代码文件。
| Features | Training ratios | Training ratios | Compression rate | Compression rate | 
| :---- | :----: | :----: | :----: | :----:|
|  | 10% | 20% | 10% | 20% |
| AlexNet |  77.32% | 79.20%  | | |
| VGGNet（VGG-11） | 86.02% | 89.96% | | |


## 5.2 局部压缩
参考论文：
- Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications[1] - 2016年发表
- Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition[2] - 2015年发表
  
具体到局部压缩方法，实验对比方案：
   - AlexNet + NWPU-RESISC45
   - 局部压缩 + AlexNet + NWPU-RESISC45

1. 选定具体局部压缩方法：Tucker decomposition[1]，对所有卷积层进行分解。
2. 实验结果：

| Features | Training ratios | Training ratios | Compression rate | Compression rate |
| :---- | :----: | :----: | :----: | :----|
|  | 10% | 20% | 10% | 20% | 
| AlexNet | 0.747266  | 0.785357  | 1.038 | 1.038 | 
| VGGNet（VGG-11） | 0.831429 | 0.882381 | 1.064 | 1.064 |

## 5.3 全局压缩
参考论文：
- Distilling the Knowledge in a Neural Network[1] - 2015年发表

具体到全局压缩方法，实验对比方案：
   - AlexNet + NWPU-RESISC45
   - 全局压缩 + AlexNet + NWPU-RESISC45

1. 选定具体全局压缩方法：KD + soft target
2. 实验结果：

压缩率：
|| Network | Training ratios | Precise | compression rate |
| :---- | :----: | :----: | :---:| :---:|
| Student | res18 | 20% | 90.53 | - | 
| Teacher | res101 | 20% | 92.09 | - | 
| T->S | res101->res18 | 20% | 90.93 | 3.8 | 

## 5.4 实验结果
### 5.4.3 实验结果比较

### 5.4.4 实验结果分析


