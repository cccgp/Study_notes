# 0. 主题
模型压缩 + 遥感图像分类
1. 分类模型 -> 遥感图像
2. 模型压缩 -> 分类网络

# 1. 背景
1. 遥感图像分类领域重要性以及应用。
2. 深度学习兴起，应用于图像分类领域，效果非常好。
3. 但是大型网络模型参数多，计算量大，部署于资源受限设备困难。
4. 采用模型压缩技术应用于大型网络模型。

# 2. 调研
1. 调研现有的模型压缩方法。
    - 现有的方法可以分为五类：
        - 网络剪枝
        - 网络量化
        - 低秩分解
        - 网络蒸馏
        - 紧性网络设计
    - 在此基础上，根据模型压缩过程中利用信息位置的不同可以分为三类：
        - 权值压缩
        - 局部压缩
        - 全局压缩
# 3. 研究思路
1. 初步探究
    - 实验：三种方法 --> 应用于 --> 遥感图像分类的网络模型。
        - 每一种方法选取具有代表性的方法即可。
    - 结果：数据集上的指标。
    - 结论：哪种方法更好？亦或者给出分析，什么场景更加合适使用哪种方案。
    - 最终目的：根据实验亦或者分析给出一种压缩方式用于后续探究。（用于解答问题：对于遥感图像分类，为什么要探究这一类压缩方法？）
2. 对比实验
    - 假设实验结果表明需要深入探究全局压缩方法。
        - 该方法用于网络模型可以得到在数据集上的指标：压缩率等。
        - 上述结果作为实验的baseline。
    - 针对数据集图像加入预处理。
        - 创新点：提出两个。
        - 结果：提升数据集评价指标。
  
# 4. 遥感图像场景分类
以下数据来源于论文：
- Remote Sensing Image Scene Classification: Benchmark and State of the Art [1] - 2017年发表 
- Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities[2] - 2020年发表

## 4.1 数据集

| Datasets | Images per class | Scene classes | Total images | Spatial resolution(m) | Image sizes | Year |
| :---- | :----: | :----: |:----: |:----: |:----: |:----: |
| UC Merced Land-Use | 100 | 21 | 2100 | 0.3 | 256 * 256 | 2010 |
| WHU-RS19 | ~50 |  19 | 1005 | up to 0.5 | 600 * 600 | 2012 | 
| SIRI-WHU | 200| 12 | 2400 | 2 | 200 * 200 | 2016 | 
| RSSCN7 | 400 | 7 | 2800 | -- | 400 * 400 | 2015 | 
| RSC11 | ~100 | 11 | 1232 | 0.2 | 512 * 512 | 2016 |
| Brazilian Coffee Scene | 1438 | 2 | 2876 | -- | 64 * 64 | 2015 |
| NWPU-RESISC45 | 700 | 45 | 31500 | ~30 to 0.2 | 256 * 256 | 2016 |

## 4.2 方法
- 手工特征方法
    - 颜色直方图
    - 纹理特征
    - GIST
    - SIFT
    - HOG
    - so on ...
- 无监督特征学习方法
    - PCA
    - k-means 聚类
    - 稀疏编码
    - 自编码器
    - so on ...
- 深度特征学习方法
    - 深度信念网络 - DBN
    - 深度玻尔兹曼机 - DBM
    - 栈式自编码器 - SAE
    - 卷积神经网络 - CNN
        - AlexNet
        - Overfeat
        - VGGNet
        - GoogLeNet
        - SPPNet
        - ResNet
        - so on ...
    - so on ...
## 4.3 代表性方法在NWPU-RESISC45的结果

| Features | Training ratios | Training ratios | 
| :---- | :----: | :----: |
|  | 10% | 20% |
| AlexNet | 76.69 +/- 0.21 | 79.85 +/- 0.13 |
| VGGNet-16 | 76.47 +/- 0.18 | 79.79 +/- 0.15 |
| GoogLeNet | 76.19 +/- 0.38 | 78.48 +/- 0.26 |

备注：
- 10%：数据集随机切分10%用于训练，90%用于测试
- 20%：数据集随机切分20%用于训练，80%用于测试
- 指标：总体精度（overall accuracy)
## 4.4 复现实验结果

复现了以下几种网络在NWPU-RESISC45数据集上的效果。（使用了在ImageNet上预训练的网络进行微调）

| Features | Training ratios | Training ratios | 
| :---- | :----: | :----: |
|  | 10% | 20% |
| AlexNet |  76.28% |  80.69% |
| VGGNet（VGG-11） | 85.38% | 89.47% |
| squeezenet | 74.76% | 79.44% | 
| resnet | 86.41% | 90.01% |

另外根据[2]内容，一些神经网络模型在该数据集上的效果如下所示：

| Features | Training ratios | Training ratios | 
| :---- | :----: | :----: |
|  | 10% | 20% |
| BoCF | 83.65%  | 84.32% |
| MSCP | 88.07% | 90.81% |
| D-CNNs | 89.22% | 91.89% | 
| IORN | 87.83% | 91.30%|
| ADSSM | 91.69% | 94.29% | 
| SF-CNN | 89.89% | 92.55% | 
| ADFF | 90.58% | 91.91% | 
| CNN-CapsNet | 89.03% | 92.10% | 
| SCCov | 89.30% | 92.10%| 
| Hydra | 92.44% | 94.51% | 

疑问：现在AlexNet等网络可以代表用于遥感图像的深度网络模型吗？虽然在数据集上的表现也非常不错。

解答：关键点是不是在于数据集？只要数据集是遥感图像数据集就可以说明？
1. 原始：AlexNet + NWPU-RESISC45
2. 之后：压缩 + AlexNet + NWPU-RESISC45

# 5. 模型压缩方法
参考论文:
- Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey[1] - 2020年发表
- A Survey of Model Compression and Acceleration for Deep Neural Networks[2] - 2017年发表

大多数压缩方法研究基于AlexNet/VGG16 + ImageNet，最近的工作集中于ResNet/GoogleNet + ImageNet，因此现在的工作应该集中于一个最简单的对比实验，后续实验可以在这方面扩展。
```
1. AlexNet + NWPU-RESISC45
2. 压缩 + AlexNet + NWPU-RESISC45
```

根据[2]的理论：
1. 三个指标：压缩率、加速比和准确率。
2. 一个好的压缩方法：准确率与原模型相同，压缩率与加速比较大。

压缩率 = 原始网络参数数量 / 压缩后网络参数数量

计算模型的参数数量代码
```
def params_count(model):
    """
    Compute the number of parameters.
    Args:
        model (model): model to count the number of parameters.
    """
    return np.sum([p.numel() for p in model.parameters()]).item()
```
## 5.1 权值压缩
参考论文：
- Learning both Weights and Connections for Efficient Neural Networks[1] - 2015年发表
- Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[2] - 2016年发表

具体到权值压缩方法，实验对比方案：
   - AlexNet + NWPU-RESISC45
   - 权值压缩 + AlexNet + NWPU-RESISC45

1. 选定具体权值压缩方法：[1]
2. 实验结果：

压缩率为85%。每一层的详细压缩信息见代码文件。
| Features | Training ratios | Training ratios | Compression rate | Compression rate | 
| :---- | :----: | :----: | :----: | :----:|
|  | 10% | 20% | 10% | 20% |
| AlexNet |  77.32% | 79.20%  | | |
| VGGNet（VGG-11） | 86.02% | 89.96% | | |


## 5.2 局部压缩
参考论文：
- Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications[1] - 2016年发表
- Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition[2] - 2015年发表
  
具体到局部压缩方法，实验对比方案：
   - AlexNet + NWPU-RESISC45
   - 局部压缩 + AlexNet + NWPU-RESISC45

1. 选定具体局部压缩方法：Tucker decomposition[1]，对所有卷积层进行分解。
2. 实验结果：

| Features | Training ratios | Training ratios | Compression rate | Compression rate |
| :---- | :----: | :----: | :----: | :----|
|  | 10% | 20% | 10% | 20% | 
| AlexNet | 0.747266  | 0.785357  | 1.038 | 1.038 | 
| VGGNet（VGG-11） | 0.831429 | 0.882381 | 1.064 | 1.064 |

## 5.3 全局压缩
参考论文：
- Distilling the Knowledge in a Neural Network[1] - 2015年发表

具体到全局压缩方法，实验对比方案：
   - AlexNet + NWPU-RESISC45
   - 全局压缩 + AlexNet + NWPU-RESISC45

1. 选定具体全局压缩方法：KD + soft target
2. 实验结果：

压缩率：
|| Network | Training ratios | Precise | compression rate |
| :---- | :----: | :----: | :---:| :---:|
| Student | res18 | 20% | 90.53 | - | 
| Teacher | res101 | 20% | 92.09 | - | 
| T->S | res101->res18 | 20% | 90.93 | 3.8 | 

## 5.4 实验结果
### 5.4.3 实验结果比较

### 5.4.4 实验结果分析

# 6. 剪枝
由最终分析，发现剪枝的效果最好，当然分析还是要说权值压缩中的细粒度剪枝。
## 6.1 方法分类
可以分为权重剪枝和神经元剪枝。
![剪枝方式分类](picture/剪枝方式对比.png?40)

 - 前者减少边的数目，后者减少节点的数目。
 - 大量内存占用：权重和激活。
 - 由于每一个神经元连接大量权重，因此神经元剪枝可以减少计算。
### 6.1.1 权重剪枝(weight pruning)
历史变迁
```
两个开篇之作
1. Optimal brain damage 1990 226
2. Second order derivatives for network pruning: Optimal brain surgeon 1993 227

证明DNN如果剪枝大量的边仍然可以有效。
普通的剪枝方案。
3. Learning both weights and connections for efficient neural network 2015 9
在前者基础之上加上了量化和哈夫曼编码
4. Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding 2015 185

结合了梯度感知生长和幅度感知修剪，这既可以用于权重，也可以用于神经元。可以深入研究，但是没有代码。
5. NeST: A neural network synthesis tool based on a grow-and-prune paradigm 2019 217

空间权重也可以转换为频域中的权重系数，然后动态剪枝不重要的系数可以帮助实现更高的压缩比，有点复杂，不懂，无代码。
6. Frequency-domain dynamic pruning for convolutional neural networks 2018 199

概率剪枝，一个用于专门硬件的方法，但是也提出了一个软件算法，看起来比较容易实施，主要针对全连接层，无代码
7. Sparsely-connected neural networks: Towardsefficient VLSI implementation of deep neuralnetworks 2016 197

压缩率 + 精度 + 能量约减,结合能量感知，值得深入研究。并且里边还讲述了结合其他方法来做，我觉得可以深挖进行改进。
8. Designingenergy-efficient convolutional neural networksusing energy-aware pruning 2017 231

除了权值本身，权值梯度也可以用相似的幅度感知剪枝来修剪，用来压缩梯度更新的，对于压缩本身倒是没什么，有代码，值得深挖，https://github.com/synxlin/deep-gradient-compression。
9. Deep gradient compression: Reducing thecommunication bandwidth for distributedtraining 2017 203

开始考虑粗粒度结构剪枝
在损失函数中加上对结构的正则，训练出一个稀疏的网络。基本算是加正则的典型了，对所有的结构都加了正则，代表作。有代码。
10. Learning structured sparsity in deep neuralnetworks 2016 201

通过迭代重排序元素稀疏权，得到块态权重稀疏性，本文提出了一种将不规则细粒度稀疏度重新排序为结构化粗粒度稀疏度的方法，以弥补模型所能得到的大稀疏度与实际加速性能较差之间的差距。它还可以帮助细粒度修剪方法实现理想的执行加速。非常可以研究，有代码。感觉原理有一些小复杂，不建议引用。
关键思想使在粗粒度剪枝之前，通过重新排序输入和输出维度，将小规模的元素聚在一起，它将不规则分布的小单元转换为密集区域，这些密集区域可因粗粒度稀疏而被删除。
11. Tetris: Tile-matching the tremendous irregular sparsity 2018 220

分组加权并用小值剪枝，充分利用了单指令多数据（SIMD）单元的数据长度
可以重点看下Node Pruning，应该会有启发
12. Scalpel: CustomizingDNN pruning to the underlying hardwareparallelism 2017 


提出了一种cnn的加速方法，从cnn中删去对输出精度影响很小的滤波器。通过去除网络中的所有滤波器及其连接的特征映射，明显降低了计算成本。在与修剪权重相比，此方法不会导致稀疏连接模式。随意看看。
思路很简单：利用l1norm衡量滤波器重要性，接着剪枝就完事了，但是重点在于首先训练出来所有层的敏感度，根据整个敏感度手工确定每一层的剪枝比例，然后两种训练方式，1.一次性剪多个层，然后重新训练。2.一次性剪一个层，然后重新训练。实验发现如果层比较不敏感，用前者即可，如果敏感，用后者更好。基本作为结构剪枝的baseline之作。代表作。
13. Pruning filters for efficient ConvNets 2016 200

- 插入一个论文，感觉非常不错，动态网络拯救，既有剪枝也有连接，俗称动态补救。有代码。https://github.com/yiwenguo/Dynamic-Network-Surgery
Dynamic Network Surgery for Efficient DNNs
上面这一篇针对权重，下面这一篇针对滤波器。
在梯度下降时修剪滤波器，成为软过滤剪枝，有代码，可以参考。
14. Softfilter pruning for accelerating deep convolutionalneural networks 2018 212
https://www.ijcai.org/Proceedings/2018/0309.pdf

作者后续的一篇工作，但是还有诸多缺点，最后也提到了改进的方向，因此我觉得这是一个非常好的扩展工作，强烈建议延续。
Asymptotic Soft Filter Pruning for Deep Convolutional Neural Networks

另外一篇后续文章，用于替换norm的，简而言之就是以往常删除小的值的滤波器，现在不一样了，用几何中值代替
Filter Pruning via Geometric Medianfor Deep Convolutional Neural Networks Acceleration
------看到这儿-----
----- 启发式方法 ----
其实如果要研究启发式方法，可以看下附录E。
在每一次迭代中，为了补偿朴素贪心剪枝所造成的损失误差，在每一次迭代中都会对最重要的滤波器进行启发式剪枝
15. Layer-compensated pruning forresource-constrained convolutional neuralnetworks 2018 213

基于启发式方法，RL允许在给定的资源或精度约束下实现传统手工修剪的自动化 
16. AMC: AutoML for model compression andacceleration on mobile devices 2018 214

将元素不规则剪枝叠加到过滤结构剪枝中，提高压缩比
17. Hybrid pruning:Thinner sparse networks for fast inference onedge devices 2018 216

利用合并的目标函数可以从零开始同时训练多个具有不同稀疏度（低层具有较高稀疏度）的稀疏网络，其中低层的权重由高层网络共享
18. NestedNet: Learningnested sparse structures in deep neural networks 2018 215

除上述启发式方法外，优化方法在提高模型精度和提高压缩比方面更受研究者的青睐，尤其是在Conv层。
提出了一种结构化稀疏学习算法(SSL)，它将权重分组到结构化形状中，并在每个组上添加最小绝对收缩和选择算子(LASSO)正则化。简而言之，就是在损失函数中加上约束。
19. Learning structured sparsity in deep neuralnetworks 2016 201
20. Sparse convolutional neuralnetworks 2015 236
21. Compression ofdeep convolutional neural networks under jointsparsity constraints 2018 221
22. Synapticstrength for convolutional neural network 2018 222

引入一个框架ADMM,24-26序号的论文都是用这个方法来剪枝
23. ADMM-NN: An algorithm-hardwareco-design framework of DNNs using alternatingdirection methods of multipliers 2019 7
24. A systematic DNN weight pruningframework using alternating direction method ofmultipliers 2018 223 
在约束条件下，通过不同的分组方案，可以产生不同的稀疏化结构
25. StructADMM: A systematic,high-efficiency framework of structured weightpruning for DNNs 2018 224
在极高的压缩比下，可以利用具有适度修剪比的逐步稀疏化来提高精度
26. StructADMM: A systematic,high-efficiency framework of structured weightpruning for DNNs 2018 225

求解优化问题的解决方法：遗传算法
27. A novel channel pruning method for deep neural network compression 2018 218
求解优化问题的解决方法：L0约束梯度下降
28. Crossbar-aware neural networkpruning 2018 219


```
### 6.1.2 神经元剪枝(Neuron pruning)
```
-------选择一些不重要的神经元方法-------
因为到了relu层时，此时负数过去就是0，因此如果可以提前知道是负数，这边就不用算了，然后提出了一系列方法，预测一下是否后边会出现负数，如果预测出来会，那么就别算了，就节约了这一点计算。
哈希搜索，看不懂在干什么。
29. Scalable andsustainable deep learning via randomized hashing 2017 241
负激活预测，分为精确模型和预测模型，预测神经元是否0输出，早点停止。
30. SnaPEA:Predictive early activation for reducingcomputation in deep convolutional neuralnetworks 2018 242
低精度估计，通过预测神经元0的输出，就不用计算了。用于加速。
31. PredictiveNet: An energy-efficient convolutionalneural network via zero prediction 2017 243
32. Prediction based execution on deep neuralnetworks 2018 244
预检查相邻像素，提出一个两阶段方法，预测0输出，用于加速。
33. Mosaic-CNN: A combined two-step zeroprediction approach to trade off accuracy andcomputation energy in convolutional neuralnetworks 2018 245
降维估计
34. Dynamic sparse graph for efficientdeep learning 2018 246

-------神经元剪枝-------
根据神经元的选择性
35. On the importance of single directions for generalization 2018 238
随机选择一些神经元丢弃，没想到吧，就是dropout
36. Dropout:A simple way to prevent neural networks fromoverfitting 2014 239
以一定的概率选择神经元丢弃
37. Adaptive dropout for trainingdeep neural networks 2013 240

形式化为优化问题 
并且还提出了一个启发式贪婪算法解决该问题
38. ThiNet: A filter levelpruning method for deep neural networkcompression 2017 205 
类似的，也解决了一个优化问题
39. Channel pruning foraccelerating very deep neural networks 2017 206
增加一个l1正则化项对于批量正则化的缩放因子
40. Learning efficient convolutionalnetworks through network slimming 2017 207
非常类似，也提了一个针对该缩放因子的正则化项
41. Rethinkingthe smaller-norm-less-informative assumption inchannel pruning of convolution layers 2018 209
提出了一种神经元剪枝方法，在每个FM中引入一个不可变比例因子的二值掩模
42. Scalpel: CustomizingDNN pruning to the underlying hardwareparallelism 2017 234
在每个激活张量后添加了一个名为autorunner的额外层，以生成类似的缩放效果。
43. AutoPruner: An end-to-endtrainable filter pruning method for efficient deepmodel inference 2018 210
泰勒展开也被用来修剪
44. Pruning convolutional neural networksfor resource efficient inference 2016 204
根据FM的条件性来估计FM的重要性，以最小化全局损失熵误差。
45. 2PFPCE: Two-phase filter pruning based onconditional entropy 2018 211
尝试通过反向传播神经元重要性得分来最小化分类前最后一个响应层的剪枝误差,基于各层的重要性估计，可以对重要性较低的FMs进行安全的剪枝
46. NISP: Pruning networks usingneuron importance score propagation 2018 208


```
