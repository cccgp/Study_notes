# 1. 配置环境
1. 安装Anaconda
   - 使用```$conda -V```检查
2. 创建虚拟环境
   - ```$conda create -n your_env_name python=x.x```
   - 键入命令后，在```## Package Plan ##```部分会提示虚拟环境所在位置
3. 激活虚拟环境
   - ```$source activate your_env_name```
4. 退出虚拟环境
   - ```$source deactivate your_env_name```
5. 安装pytorch
   - 检查cuda版本：`$nvidia-smi`，结果中的`CUDA Version`即cuda版本
   - https://pytorch.org/ ，按照网站中的命令安装即可
   - 验证pytorch版本
     - ```$python```
     -  ```
        import torch
        print(torch.__version__)
        print(torch.version.cuda)
        print(torch.backends.cudnn.version())
        ```
6. 自动登陆ssh和sftp
   - 由于每次登陆ssh后还需要做一些工作才方便使用，于是配置了shell脚本，在iTerm2启动之前执行，点击图标之后，即可自动做完所有初始化工作，直接利于使用ssh和sftp。        
# 2. Network + NWPU-RESISC45
微调指南：
https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html
## 2.1 AlexNet 
   - src/alextNet_20%.py
   - 参数设置：切分比例:20%，Epoch:15,batch size = 128,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.806865
   - src/alextNet_10%.py
   - 参数设置：切分比例:10%，Epoch:15,batch size = 128,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.762822

## 2.2 VGG-11
   - src/vggNet_20%.py
   - 参数设置：切分比例:20%，Epoch:35,batch size = 50,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.894683
   - src/vggNet_10%.py
   - 参数设置：切分比例:10%，Epoch:15,batch size = 50,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.853757

## 2.3 squeezenet
   - src/squeezenet_20%.py
   - 参数设置：切分比例:20%，Epoch:15,batch size = 120,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.794405
   - src/squeezenet_10%.py
   - 参数设置：切分比例:10%，Epoch:15,batch size = 120,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.747619

## 2.4 resnet
   - src/resNet_20%.py
   - 参数设置：切分比例:20%，Epoch:100,batch size = 120,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.900119
   - src/resNet_10%.py
   - 参数设置：切分比例:10%，Epoch:100,batch size = 120,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.864092

# 3. Model compression
指定GPU可见，方便指定GPU训练
- $CUDA_VISIBLE_DEVICES=1 python my_script.py

- 使用sequential搭建模型，分为features及classifier两个部分，可以使用model.classifier[1]引用特定层

- 杀死进程，释放显存，例如运行包含train_imagenet文件的进程，ps -ef | grep pruning | grep -v grep | cut -c 9-15 | xargs kill -9

  
## 3.1 权值压缩 + NWPU-RESISC45
- 剪枝指南：https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#global-pruning
### 3.1.1 AlexNet
   - pruning/pruning_alexNet_10%.py
   - 参数设置：切分比例:10%，Epoch:100,batch size = 128,learning rate = 0.001,momentum = 0.9 
   - 实验结果：0.773157
   - 压缩率：0.85
   - pruning/pruning_alexNet_20%.py
   - 参数设置：切分比例:20%，Epoch:50,batch size = 128,learning rate = 0.001,momentum = 0.9 
   - 实验结果：0.791984
   - 压缩率：0.85 
### 3.1.2 VGG
   - pruning/pruning_vggNet_10%.py
   - 参数设置：切分比例:10%，Epoch:100,batch size = 50,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.860176
   - 压缩率：0.85
   - pruning/pruning_vggNet_20%.py
   - 参数设置：切分比例:20%，Epoch:100,batch size = 50,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.899563
   - 压缩率：0.85 

### 3.1.3 方法说明
- 实验中使用的TORCH.NN.UTILS.PRUNE.GLOBAL_UNSTRUCTURED
  https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html
- 具体使用的方法prune.L1Unstructured
  https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html
- ```
    prune.global_unstructured(
         parameters_to_prune,
         pruning_method=prune.L1Unstructured,
         amount=0.85
    )
## 3.2 局部压缩 + NWPU-RESISC45
- 分解指南：https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning
- 代码参考：https://github.com/jacobgil/pytorch-tensor-decompositions

### 3.2.1 AlexNet
   - low-rank/pytorch-tensor-decomposition/alex_10%_v2.py
   - 参数设置：切分比例:10%，Epoch:50,batch size = 128,learning rate = 0.001,momentum = 0.9 
   - 实验结果：0.747266
   - 压缩率：1.038
   - pruning/alex_20%_v2.py
   - 参数设置：切分比例:20%，Epoch:50,batch size = 128,learning rate = 0.001,momentum = 0.9 
   - 实验结果：0.785357
   - 压缩率：1.038
### 3.2.2 VGG
   - low-rank/pytorch-tensor-decomposition/vgg_10%_v2.py
   - 参数设置：切分比例:10%，Epoch:50,batch size = 50,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.831429 
   - 压缩率：1.064
   - low-rank/pytorch-tensor-decomposition/vgg_20%_v2.pyy
   - 参数设置：切分比例:20%，Epoch:50,batch size = 50,learning rate = 0.001,momentum = 0.9
   - 实验结果：0.882381
   - 压缩率：1.064
### 3.2.3 方法说明
  - 使用了Tucker Decomposition
  - 另外有CP分解，也可以尝试
  
## 3.3. 全局压缩 + NWPU-RESISC45
- 蒸馏训练指南：https://github.com/AberHu/Knowledge-Distillation-Zoo

### 3.3.1 resNet101->resNet18
   - Baseline:resNet18 + NR + 20%
   - 训练指令
      ```
          CUDA_VISIBLE_DEVICES = 0 python -u train_base.py \
                               --save_root "./results/base/" \
                               --data_name a \
                               --num_class 45 \
                               --net_name resnet18 \
                               --note base-NR-r18 \
                               --batch_size 50 \
                               --lr 0.001 \
                               --epochs 500
   - 实验结果：90.53%
   - Teacher:resNet101 + NR + 20%
   - 训练指令
      ```
      CUDA_VISIBLE_DEVICES=1 python -u train_base.py \
                           --save_root "./results/base/" \
                           --data_name a \
                           --num_class 45 \
                           --net_name resnet101 \
                           --note base-NR-r101 \
                           --batch_size 50 \
                           --lr 0.001 \
                           --epochs 500
   - 实验结果：92.09%
   - Teacher -> Student : resNet101 -> resNet18
   - 训练指令
      ```
      CUDA_VISIBLE_DEVICES=0 python -u train_kd.py \
                           --save_root "./results/st/" \
                           --t_model "./results/base/base-NR-r101/model_best.pth.tar" \
                           --s_init "./results/base/base-NR-r18/initial_r18.pth.tar" \
                           --data_name a \
                           --num_class 45 \
                           --t_name resnet101 \
                           --s_name resnet18 \
                           --kd_mode st \
                           --lambda_kd 0.1 \
                           --T 4.0 \
                           --note st-NR-r101-r18
   - 实验结果：90.93%
   - 压缩率：3.8

### 3.3.2 方法说明
  - 使用了Soft Target
  - 另外还有很多方法可以尝试。

# 4. sfp + NWPU-RESISC45
## 4.1 baseline 
已经复现完毕。方法有效，理论也没有问题。继续系统性对比实验。